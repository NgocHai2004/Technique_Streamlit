import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model Qwen
model_name = "Qwen/Qwen3-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

st.title("Gi·∫£i b√†i t·∫≠p V·∫≠t L√Ω v·ªõi Qwen3-0.6B")

# Nh·∫≠p b√†i to√°n
user_prompt = st.text_area("1+1 =?")

if st.button("Gi·∫£i b√†i"):
    if user_prompt.strip() == "":
        st.warning("Vui l√≤ng nh·∫≠p b√†i to√°n!")
    else:
        messages = [
            {"role": "system", "content": "B·∫°n l√† m·ªôt chuy√™n gia gi·∫£i b√†i t·∫≠p v·∫≠t l√Ω. Gi·∫£i b√†i t·∫≠p v·∫≠t l√Ω sau ƒë√¢y t·ª´ng b∆∞·ªõc. Th·∫ø s·ªë v√† k·∫øt lu·∫≠n."},
            {"role": "user", "content": user_prompt}
        ]

        # Apply chat template
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Encode text
        input_ids = tokenizer(text, return_tensors="pt").to(model.device)

        # Generate response
        output_ids = model.generate(
            **input_ids,
            max_new_tokens=512,
            temperature=0.7
        )

        # Decode output
        response = tokenizer.decode(output_ids[0][input_ids['input_ids'].shape[-1]:], skip_special_tokens=True)

        st.markdown("### üìñ L·ªùi gi·∫£i:")
        st.write(response)
